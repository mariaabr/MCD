{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "375aacc5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Técnicas Matemáticas para Big Data - Project 02\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c68c",
   "metadata": {},
   "source": [
    "GROUP NN:\n",
    "- Student 1 - Nº xxxxx - ??% Work Participation\n",
    "- Student 2 - Nº xxxxx - ??% Work Participation\n",
    "- Student 3 - Nº xxxxx - ??% Work Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66fb045",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 1. Introduction to the problem of study [1,0 valor]\n",
    "\n",
    "Predicting our opponent's moves in match situations is a complex but interesting task to study. It encourages the players to identify possible strategies that their opponent may be trying and to apply a counter-move in order to win the game, or at least, defend themselves.\n",
    "\n",
    "The callenge in predicting this moves lies in the different probabilities each move has depending on the last move. The biggest problem is that these odds are hidden and the player can not see or calculate these odds, all they get is the final result, the observation of their opponent's chosen move. These games are characterised by interections between players, where every decision influences the final outcome.\n",
    "\n",
    "This study aims not only to predict the opponet's next move, but also to assess the effectiveness of applying the Hidden Markov Model in contexts of different levels of strategic complexity. The Hidden Markov Models (HMMs) belong to the statistical models that model the observed data as a series of events or data, presuming that each observation is conditioned on the state of a Hidden Markov Chain [1]. Our goal is to infer which strategy the opponent is adopting at each moment so that we can increase the player’s odds, using Markov games combined with Hidden Markov Model. The HMM assumes that the system evolves sequentially, and the next state depends only on the current state ( Markov property), but the states are hidden, we do not know directly what state the system is in, but we can infer it based on observations, the move made.\n",
    "\n",
    "Assessing this problem is interesting for several reasons:\n",
    "### Theoretical Relevance\n",
    "\n",
    "1. **Understanding human patterns:** Humans often follow predictable patterns, even in games of chance, and understanding these patterns can reveal insights into decision-making, strategy and learning.\n",
    "\n",
    "2. **Testing the applicability of HMM:** Hidden Markov Models are powerful tools for dealing with problems where states are not directly observable. Testing their effectiveness in simple scenarios such as games is an essential step before applying them to more complex contexts.\n",
    "\n",
    "### Practical Impact\n",
    "\n",
    "1. **Behaviour prediction:** The ability to predict opponent behaviour can be used to improve performance in games.\n",
    "\n",
    "2. **Simulating opponents:** Using HMMs to model players allows the creation of simulated opponents that can be used to train players or test systems.\n",
    "\n",
    "3. **Interdisciplinarity:** The problem connects statistics, machine learning, game theory and psychology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af2342",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. Brief and general description of the approach and methods used [1,5 valor]\n",
    "\n",
    "In a Hidden Markov Model, as already explained, we have a set of hidden states that are exactly like a normal Markov Chain, and a set of observable states. We do not know the hidden chain, neither the actual state, but we know the actual observation. This allows us to obtain information about the hidden chain after several observations.\n",
    "\n",
    "We will apply the Hidden Markov Method to a specific game, the Rock Paper Scissors. Other good game to test HMM in this context would be the Tic Tac Toe, slightly more complex.\n",
    "Each of these games belongs to the category of repeated games. A repeated game is a situation in which a basic game is played several times in sequence by the same players. The decisions in each round can influence future strategies and actions in each stage of the game have immediate payoffs and future consequences [3].\n",
    "\n",
    "The main difference between these games:\n",
    "- **Rock-paper-scissors:** Focused on random choices to neutralise the opponent.\n",
    "\n",
    "- **Tic-tac-toe:** More strategic, with tactics to ensure draws or wins.\n",
    "\n",
    "Both are classic examples for exploring game theory in the context of repeated games, as they illustrate the impact of past interactions on players' future behaviour.\n",
    "\n",
    "The initial state is chosen by a random probability _p_ where only the player who is playing knows his state. The other player knows the possible states, but does not know the actual state. After each round, both players know the action of each one, and play again.\n",
    "\n",
    "### Game simulation\n",
    "To apply the HMM to each of the chosen games we need to simulate a set of possible games. These simulations will produce sequences of possible moves and observations that will serve as input for the HMM. In the case of Rock Paper Scissors, each move will be one of three possible options. In Tic Tac Toe, the moves will could represented by numbers in a 3x3 matrix, but we are not exploring it in this study.\n",
    "\n",
    "### Definition of states and probabilities\n",
    "The HMM will be built based on the patterns observed in the simulations.\n",
    "\n",
    "In Rock Paper Scissors, the states can represent the opponent's underlying strategies (tendency to repeat moves or switch). The transition probabilities between states will be adjusted based on the simulated sequences.\n",
    "\n",
    "To determine this probabilities some methods were applied:\n",
    "\n",
    "**NESTA PARTE É PRECISO HAVER CONCORDÂNCIA COM O CÓDIGO UMA VEZ QUE RESUMO A ABORDAGEM: DEPOIS É PRECISO ESCREVER COMO FORAM DEFINIDAS AS PROBABILIDADES E SE FORAM USADOS ALGUNS MÉTODOS ESPECÍFICOS PARA CHEGAR À CONCLUSÃO DOS MELHORES VALORES OU NÃO**\n",
    "\n",
    "### Implementation of the Hidden Markov Model\n",
    "- **Training:** Using the sequences of simulated moves, the HMM will adjust the parameters of transition probability between states and emission (observations).\n",
    "\n",
    "- **Prediction:** The model will be tested to predict the future moves of two opponent profiles: a ‘good’ one (which follows optimised strategies) and a ‘bad’ one (with inconsistent or random strategies).\n",
    "\n",
    "**ISTO TAMBÉM SE APLICA? a previsão tem os dois casos do exemplo, quando se pensou em fazer um exemplo pior e um melhor para retirar conclusões acerca da aplicação do hmm**\n",
    "\n",
    "### Performance analysis\n",
    "The results will be evaluated in terms of predictive accuracy, computational efficiency and applicability in different contexts (whether it can be applied in more complex contexts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e324615",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 3. Brief History and literature review of the problem and methods/algorithms [1,5 valor]\n",
    "\n",
    "**Game theory** is the study of decision problems in which there are multiple decision makers and the quality of a decision maker's choice depends on both that choice and the choices of others [6]. This has been studied predominantly as a modeling paradigm in mathematical social sciences. In brief, it is the study of how and why individuals and entities (the players) make decisions about their situations. In some aspects, it is the science of strategy. **Game theory** is a model for making decisions that weigh the benefits of a choice along with the interaction between individuals. In general, it looks at relationships and tries to predict the optimal decisions people will make.\n",
    "**Game theory** was formalised by John von Neumann and Oskar Morgenstern in the book _Theory of Games and Economic Behavior (1944)_ [5].\n",
    "\n",
    "The main goal of game theory is to explain the strategic actions of players in a given situation. These situations are composed of different states, possible observations and probabilities between states. Any of these situations, with two or more players, involves known moves or identified consequences, which makes it possible to use game theory to help determine or predict the most likely outcomes. The key to game theory is that one player's payoff is contingent on the strategy implemented by the other player [5].\n",
    "\n",
    "Basic components of game theory:\n",
    "\n",
    "- **Players:** Agents who make decisions.\n",
    "- **Plays:** The possible actions that players can take.\n",
    "- **Payoffs:** The results for each combination of moves, which can be positive (wins), negative (losses) or neutral.\n",
    "- **Equilibrium:** The point in a game where both players have made their decisions and an outcome is reached.\n",
    "- **Strategies:** The set of rules or plans used by players to decide their moves.\n",
    "- **Types of games:**\n",
    "  1. Zero-sum games: One player's win is equal to the other's loss, as in Rock-Paper-Scissors.\n",
    "  2. Non-zero-sum games: Players can benefit each other.\n",
    "  3. Repeated games: The same game is played several times, like Tic Tac Toe.\n",
    "\n",
    "Since the beginning of the study of game theory, it has been applied to countless situations. It is widely documented in game theory and behavioural psychology literature that humans tend to exhibit predictable patterns, such as repeating strategies after success or changing after failure, which means that people easly adapt to others in adverse contexts [8][11].\n",
    "\n",
    "A classic example is the study _‘Social cycling and conditional responses in the Rock-Paper-Scissors game’ (2014)_ by Zhou et al., published in Scientific Reports in 2014. This work analyses how Rock-Paper-Scissors players deviate from randomness and adopt cyclical behaviours that can be exploited. A classic game theory of infinite rationality predicts the Nash Equilibrium (NE) state with every player randomizing her choices to avoid being exploited, while evolutionary game theory of bounded rationality in general predicts persistent cyclic motions. In this study, they observe population-level cyclic motions under the traditional random pairwise-matching protocol [9].\n",
    "\n",
    "Other studies also show that these sequence effects are common because humans have difficulty producing truly random series. This observation is based on previous work at the intersection of cognitive psychology and game theory. In addition, many individuals find it difficult to remember the different moves they have already made and others give in to the desire to win by always trying to think of what their opponent will play in order to try to predict what has been played in order to win [12].\n",
    "\n",
    "Other work on repetitive games and the application of game theory to them, highlights how players adjust their strategies based on previous interactions. Axelrod (1984) explored how cooperation and rivalry emerge in repeated games, with implications for predictive modelling.\n",
    "This work is widely cited in the literature on strategic behaviour and learning in games. Robert Axelrod's study, entitled _The Evolution of Cooperation (1984)_, was crucial to understand how co-operative strategies can emerge in competitive contexts and how these strategies can be adjusted [10].\n",
    "\n",
    "Some fundamental principles of Axelrod's study are the existence of **Tit-for-Tat Strategies** and the **Payback Condition**.\n",
    "- **Tit-for-Tat Strategies:** Axelrod found that a simple ‘tit-for-tat’ strategy (give as much as you get) was highly effective in promoting cooperation. This behaviour starts with cooperation and then copies the opponent's behaviour from the previous round.\n",
    "- **Payback Condition:** Cooperation is favoured in repeated games because the future impact of today's actions affects tomorrow's behaviour. Players learn that **betraying in the short term** can **result in losses in the long term**.\n",
    "\n",
    "These fundamentals principles can be useful for inferring and determining the transition probabilities between states in the Hidden Marvok Model, since Tit-for-Tat Strategies and Payback Conditions directly influence how players adjust their choices based on the history of interactions.\n",
    "\n",
    "### Application of HMM in the context of game theory\n",
    "Once again, the combination of game theory with Hidden Markov Models (HMMs) is a promising approach to modelling and predicting this player behaviour in repeated scenarios [2]. The hidden states of the HMM can represent a player's underlying strategies (attacking, defending, alternating moves), the observations are consequences of these strategies (of the states), and the model can adjust transition probabilities between states and predict future moves based on observed patterns.\n",
    "\n",
    "#### Why using HMM in this context?\n",
    "Since players' decisions are influenced by hidden trends and patterns that are difficult to observe directly the HMM can deal with this hidden states. It is also suitable for sequential problems, like a game, where each decision depends on the current state and previous observations. And finally, the HMM can be applied to both simple games, such as Rock-Paper-Scissors, and a more strategic one, such as Tic Tac Toe.\n",
    "\n",
    "HMMs have been widely applied to model behaviour in other games, more complex. In games like poker, for example, they are used to predict opponents' hands based on observed bets. In chess, HMMs help identify playing styles or tactics based on moves [13][14]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889a980",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 4. About the main method/algorithm used [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52795c65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Python imports and global configurations [0,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f3deb",
   "metadata": {},
   "source": [
    "### Install and import the necessary libraries to compute the Bayesian Network and perform other methods  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5177dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install seaborn\n",
    "# %pip install matplotlib\n",
    "# %pip install numpy\n",
    "# %pip install pomegranate\n",
    "# %pip install torch\n",
    "# %pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb739aae",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 6. Dataset and variables explanation [1,5 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5703771",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 7. Main code as possible solution to the problem [1,5 valor] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15554",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 8. Analysis of Example 1 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7810c",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 9. Analysis of Example 2 [3,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821172",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 10. Pros and cons of the approach [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60974d68",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 11. Future improvements [2,0 valor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1eb04",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div style=\"text-align: center;\">\n",
    "    <br><br>\n",
    "    <p style=\"font-size: 40px;\">References [1,0 valor]</p>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "[1] Hidden Markov Model. (n.d.). ScienceDirect. Retrieved November 16, 2024, from https://www.sciencedirect.com/topics/medicine-and-dentistry/hidden-markov-model\n",
    "\n",
    "[2] Bengio, Y., Courville, A., & Vincent, P. (2014). Representation learning: A review and new perspectives. arXiv preprint arXiv:1404.0086. Retrieved November 16, 2024, from https://arxiv.org/pdf/1404.0086\n",
    "\n",
    "[3] Repeated Game. (n.d.). ScienceDirect. Retrieved November 16, 2024, from https://www.sciencedirect.com/topics/social-sciences/repeated-game\n",
    "\n",
    "[4] Repeated game. (n.d.). Wikipedia. Retrieved November 16, 2024, from https://en.wikipedia.org/wiki/Repeated_game\n",
    "\n",
    "[5] Game Theory. (n.d.). Investopedia. Retrieved November 16, 2024, from https://www.investopedia.com/terms/g/gametheory.asp\n",
    "\n",
    "[6] Boyd, S., Candès, E. J., & Boyd, D. (2021). Foundations and applications of control systems theory. Annual Reviews in Control, 60. Retrieved November 16, 2024, from https://www.annualreviews.org/content/journals/10.1146/annurev-control-060117-105102\n",
    "\n",
    "[7] Author(s). (n.d.). Title of the paper. Journal of Civil Engineering and Project Management. Retrieved November 16, 2024, from https://www.jcepm.com/article_104442_336761f3c9cf58702e352cc3d8d5ab20.pdf\n",
    "\n",
    "[8] Author(s). (n.d.). Title of the paper. Cognition. Retrieved November 16, 2024, from https://www.sciencedirect.com/science/article/abs/pii/S0010028524000252\n",
    "\n",
    "[9] Neal, R. M. (2014). Software for flexible Bayesian modeling. arXiv preprint arXiv:1404.5199. Retrieved November 16, 2024, from https://arxiv.org/abs/1404.5199\n",
    "\n",
    "[10] Axelrod, R. (n.d.). The Evolution of Cooperation. Stanford University. Retrieved November 16, 2024, from https://ee.stanford.edu/~hellman/Breakthrough/book/pdfs/axelrod.pdf\n",
    "\n",
    "[11] Cohen, J. D., McClure, S. M., & Yu, A. J. (2007). Should I stay or should I go? How the human brain manages the trade-off between exploitation and exploration. Philosophical Transactions of the Royal Society B: Biological Sciences, 362(1481), 933–942. Retrieved November 16, 2024, from https://pmc.ncbi.nlm.nih.gov/articles/PMC2430007/\n",
    "\n",
    "[12] Nickerson, R. S. (2002). The production and perception of randomness. Psychological Review, 109(2), 330–357. Retrieved November 16, 2024, from https://doi.org/10.1037/0033-295X.109.2.330\n",
    "\n",
    "[13] Dreżewski, R., & Wątor, G. (2021). Chess as sequential data in a chess match outcome prediction using deep learning with various chessboard representations. Procedia Computer Science, 192, 1330-1339. Retrieved November 16, 2024, from https://doi.org/10.1016/j.procs.2021.08.180\n",
    "\n",
    "[14] Sweeney, N., & Sinclair, D. (n.d.). Applying reinforcement learning to poker. School of Computing, Dublin City University. Retrieved November 16, 2024, from https://d1wqtxts1xzle7.cloudfront.net/32426115/sweeney-libre.pdf?1391613446=&response-content-disposition=inline%3B+filename%3DApplying_Reinforcement_Learning_to_Poker.pdf&Expires=1731958720&Signature=PsIK~SLZddxm8HuSzSLfJ6-PexKf2OHoehg82J54qduYCGJnVavfHdCbnrXqVuC33V089X3gWzSiMtXwnwlxJvMT~ppP0bumZRKl~vGEOxv17s8zHU4EOeVb~ebtdTBM4FmsOxcwmhCIwo9qwgq~~dn7xqDWd5~68LI7544fZXQMJAK9vCPf7ZnRE~nFH-SC8B7F-Br8wEpdU7Ab37HXjv3TSFY8fqzRIUuGXSNsLPaKiaFFX7bFiTsjBDxJxjnHMPgxWmCQLtfeQiot4VhXLqS3CGa29LoWmTFU~PGsB15s2JObyVrT9M~ZIhbyt8AHHAf7AL9EDnPVC9lN7rfZUQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
