{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLSD: Spark Intro\n",
    "#### Maria Rafaela Alves Abrunhosa 107658\n",
    "**13th March 2025**\n",
    "\n",
    "This exercise is an introduction to Spark and has three subtasks:\n",
    "- count the occurrences of different words in a text file (lusiadas.txt)\n",
    "- find the most common biowords (ignoring words with less than 3 letters)\n",
    "- calculate the number of unique words that begin with each letter of the alphabet (also ignoring words with less than 3 letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyspark\n",
    "\n",
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Count the occurrences of different words in a text file (lusiadas.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create a spark session\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount Distinct Words\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# create a dataframe using lusiadas.txt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m lusiadas \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlusiadas.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession.builder.appName(\"Count Distinct Words\").getOrCreate()\n",
    "\n",
    "# create a dataframe using lusiadas.txt\n",
    "lusiadas = spark.read.text(\"lusiadas.txt\").rdd.map(lambda row: row[0])\n",
    "\n",
    "# clean text, split in words and convert to lower case\n",
    "lusiadasWords = lusiadas.flatMap(lambda line: line.lower().split()) # flatMap necessary to split the spark lines in words\n",
    "\n",
    "# remove punctionation\n",
    "lusiadasWords = lusiadasWords.map(lambda word: ''.join(filter(str.isalpha, word))) # verify if the characters are alphabetic\n",
    "\n",
    "# apply map reduce with spark\n",
    "lusiadasCount = lusiadasWords.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) # create key-value pairs and sum the same words\n",
    "\n",
    "# convert to df\n",
    "lusiadasCountdf = lusiadasCount.toDF([\"Word\", \"Count\"])\n",
    "lusiadasCountdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Find the most common biowords (ignoring words with less than 3 letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Biword=Row(_1='camões', _2='lusíadas'), Count=1),\n",
       " Row(Biword=Row(_1='lusitana', _2='por'), Count=2),\n",
       " Row(Biword=Row(_1='antes', _2='navegados'), Count=1),\n",
       " Row(Biword=Row(_1='passaram', _2='ainda'), Count=1),\n",
       " Row(Biword=Row(_1='perigos', _2='guerras'), Count=1),\n",
       " Row(Biword=Row(_1='esforçados', _2='mais'), Count=1),\n",
       " Row(Biword=Row(_1='que', _2='prometia'), Count=1),\n",
       " Row(Biword=Row(_1='sublimaram', _2='também'), Count=1),\n",
       " Row(Biword=Row(_1='memórias', _2='gloriosas'), Count=1),\n",
       " Row(Biword=Row(_1='que', _2='foram'), Count=3),\n",
       " Row(Biword=Row(_1='aqueles', _2='que'), Count=10),\n",
       " Row(Biword=Row(_1='valerosas', _2='vão'), Count=1),\n",
       " Row(Biword=Row(_1='libertando', _2='cantando'), Count=1),\n",
       " Row(Biword=Row(_1='tanto', _2='ajudar'), Count=1),\n",
       " Row(Biword=Row(_1='engenho', _2='arte'), Count=1),\n",
       " Row(Biword=Row(_1='navegações', _2='grandes'), Count=1),\n",
       " Row(Biword=Row(_1='que', _2='fizeram'), Count=3),\n",
       " Row(Biword=Row(_1='calese', _2='alexandro'), Count=1),\n",
       " Row(Biword=Row(_1='que', _2='canto'), Count=2),\n",
       " Row(Biword=Row(_1='peito', _2='ilustre'), Count=1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a spark session\n",
    "spark2 = SparkSession.builder.appName(\"Most Common biwords\").getOrCreate()\n",
    "\n",
    "# use lusiadasWords already cleaned from exercise 2\n",
    "lusiadasWords2 = lusiadas.flatMap(lambda line: line.lower().split()) # flatMap necessary to split the spark lines in words\n",
    "lusiadasWords2 = lusiadasWords2.map(lambda word: ''.join(filter(str.isalpha, word))) # verify if the characters are alphabetic\n",
    "lusiadasWords2 = lusiadasWords2.filter(lambda word: len(word) >= 3) # ignore words with less than 3 letters\n",
    "\n",
    "# get words with index\n",
    "wordsIndex = lusiadasWords2.zipWithIndex().map(lambda wI: (wI[1], wI[0])) # create tuples like (index, word)\n",
    "wordsIndex2 = wordsIndex.map(lambda newtup: (newtup[0] - 1, newtup[1])) # create a new tuples, modify the original rdd by reducing the index by 1\n",
    "biwordsJoin = wordsIndex.join(wordsIndex2) # compare the two rdds and join the elements with the same key, the indexes, the current word with the previous word\n",
    "\n",
    "biwords = biwordsJoin.map(lambda biword: (biword[1], 1)) # create key-value pairs with biwords\n",
    "biwords = biwords.reduceByKey(lambda a, b: a + b) # sum the same biwords\n",
    "\n",
    "# convert to df\n",
    "lusiadasCountdf2 = biwords.toDF([\"Biword\", \"Count\"])\n",
    "lusiadasCountdf2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "Calculate the number of unique words that begin with each letter of the alphabet (also ignoring words with less than 3 letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/14 12:25:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Unique Word Letter Alphabet='l', Count=285),\n",
       " Row(Unique Word Letter Alphabet='v', Count=398),\n",
       " Row(Unique Word Letter Alphabet='c', Count=999),\n",
       " Row(Unique Word Letter Alphabet='p', Count=789),\n",
       " Row(Unique Word Letter Alphabet='a', Count=1072),\n",
       " Row(Unique Word Letter Alphabet='b', Count=227),\n",
       " Row(Unique Word Letter Alphabet='q', Count=93),\n",
       " Row(Unique Word Letter Alphabet='o', Count=199),\n",
       " Row(Unique Word Letter Alphabet='m', Count=516),\n",
       " Row(Unique Word Letter Alphabet='n', Count=223),\n",
       " Row(Unique Word Letter Alphabet='t', Count=477),\n",
       " Row(Unique Word Letter Alphabet='g', Count=232),\n",
       " Row(Unique Word Letter Alphabet='e', Count=679),\n",
       " Row(Unique Word Letter Alphabet='f', Count=454),\n",
       " Row(Unique Word Letter Alphabet='h', Count=145),\n",
       " Row(Unique Word Letter Alphabet='r', Count=414),\n",
       " Row(Unique Word Letter Alphabet='s', Count=599),\n",
       " Row(Unique Word Letter Alphabet='d', Count=700),\n",
       " Row(Unique Word Letter Alphabet='i', Count=370),\n",
       " Row(Unique Word Letter Alphabet='á', Count=22)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a spark session\n",
    "spark3 = SparkSession.builder.appName(\"Unique Words - each letter of the alphabet\").getOrCreate()\n",
    "\n",
    "# use lusiadasWords already cleaned from exercise 2\n",
    "lusiadasWords3 = lusiadas.flatMap(lambda line: line.lower().split()) # flatMap necessary to split the spark lines in words\n",
    "lusiadasWords3 = lusiadasWords3.map(lambda word: ''.join(filter(str.isalpha, word))) # verify if the characters are alphabetic\n",
    "lusiadasWords3 = lusiadasWords3.filter(lambda word: len(word) >= 3) # ignore words with less than 3 letters\n",
    "\n",
    "# get first letter and map unique words\n",
    "uniqueWordsLetter = lusiadasWords3.distinct().map(lambda word: (word[0], 1)) # create key-value pairs with unique words per letter\n",
    "uniqueWordsLetterCount = uniqueWordsLetter.reduceByKey(lambda a, b: a + b) # count unique words per letter\n",
    "\n",
    "# convert to df\n",
    "lusiadasCountdf3 = uniqueWordsLetterCount.toDF([\"Unique Words Letter Alphabet\", \"Count\"])\n",
    "lusiadasCountdf3.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
